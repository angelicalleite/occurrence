#Morphline that reads records from Hive RC files and transform those records into SolrInputDocuments
morphlines : [
  {
    id : occurrenceMorphlineImporter

    # Import all morphline commands in these java packages and their subpackages.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]

    commands : [
      {
        readRCFile {
          readMode: row
          includeMetaData: false
          columns: [

            {
              inputField: 408
              outputField: dataset_key
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 422
              outputField: taxon_key
              writableClass: "org.apache.hadoop.io.ArrayWritable"
            }
            {
              inputField: 275
              outputField: basis_of_record
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 407
              outputField: year
              writableClass: "org.apache.hadoop.io.IntWritable"
            }
            {
              inputField: 357
              outputField: month
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 412
              outputField: elevation
              writableClass: "org.apache.hadoop.io.DoubleWritable"
            }
            {
              inputField: 414
              outputField: depth
              writableClass: "org.apache.hadoop.io.DoubleWritable"
            }
            {
              inputField: 278
              outputField: catalog_number
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 383
              outputField: recorded_by
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 380
              outputField: record_number
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 336
              outputField: institution_code
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 280
              outputField: collection_code
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 283
              outputField: country
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 282
              outputField: continent
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 409
              outputField: publishing_country
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 420
              outputField: spatial_issues
              writableClass: "org.apache.hadoop.io.BooleanWritable"
            }
            {
              inputField: 419
              outputField: has_coordinate
              writableClass: "org.apache.hadoop.io.BooleanWritable"
            }
            {
              inputField: 290
              outputField: latitude
              writableClass: "org.apache.hadoop.io.DoubleWritable"
            }
            {
              inputField: 291
              outputField: longitude
              writableClass: "org.apache.hadoop.io.DoubleWritable"
            }
            {
              inputField: 301
              outputField: event_date
              writableClass: "org.apache.hadoop.io.LongWritable"
            }
            {
              inputField: 410
              outputField: last_interpreted
              writableClass: "org.apache.hadoop.io.LongWritable"
            }
            {
              inputField: 397
              outputField: type_status
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 436
              outputField: media_type
              writableClass: "org.apache.hadoop.io.ArrayWritable"
            }
            {
              inputField: 418
              outputField: issue
              writableClass: "org.apache.hadoop.io.ArrayWritable"
            }
            {
              inputField: 300
              outputField: establishment_means
              writableClass: "org.apache.hadoop.io.Text"
            }
            {
              inputField: 366
              outputField: occurrence_id
              writableClass: "org.apache.hadoop.io.Text"
            }
          ]
        }
      }
      {
        java {
          imports : "import java.util.*;"
          code: """
           List<Long> eventDateAsLong = record.get("event_date");
           if(!eventDateAsLong.isEmpty()) {
             Date dateValue = new Date((Long)eventDateAsLong.get(0));
             record.removeAll("event_date");
             record.put("event_date",dateValue);
           }
           List<Long> lastInterpretedAsLong = record.get("last_interpreted");
           if(!lastInterpretedAsLong.isEmpty()) {
             Date lastInterpretedValue = new Date((Long)lastInterpretedAsLong.get(0));
             record.removeAll("last_interpreted");
             record.put("last_interpreted",lastInterpretedValue);
           }

           return child.process(record);
               """
        }
      }
    ]
  }
]
